import gzip, json, os, requests, csv, sys, time
from pathlib import Path
from jsonschema import validate
from typing import Optional, Dict, Any
import logging

# [ì„¤ì •] ì¸í”„ë¼ ë° ê²½ë¡œ
INDEX_URL = "https://www.gutenberg.org/cache/epub/feeds/pg_catalog.csv"
STATE_PATH = Path("state.json")
OUT_DIR = Path("products")
OUT_DIR.mkdir(exist_ok=True)
MAX_BOOKS = 5

# [ë³´ì•ˆ] GitHub Secretsì—ì„œ API í‚¤ ë¡œë“œ
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# [ë¦¬ìŠ¤í¬ ì œì–´] Rate Limit ë° ì¬ì‹œë„ ì„¤ì •
RATE_LIMIT_RPM = 15  # Gemini ë¬´ë£Œ í‹°ì–´
RATE_LIMIT_DELAY = 60 / RATE_LIMIT_RPM + 0.5  # 4.5ì´ˆ (ì•ˆì „ ì—¬ìœ )
MAX_RETRIES = 3
RETRY_BACKOFF_BASE = 2  # ì§€ìˆ˜ ë°±ì˜¤í”„ ë² ì´ìŠ¤

# [ë¡œê¹…] êµ¬ì¡°í™”ëœ ì—ëŸ¬ ì¶”ì 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# [HG2] Schema ë¡œë“œ
try:
    SCHEMA = json.loads(Path("schema.json").read_text(encoding="utf-8"))
except Exception as e:
    logger.warning(f"Schema load failed: {e}, using fallback")
    SCHEMA = {"type": "object", "required": ["book_id"]}

def load_processed_ids():
    """ìƒíƒœ ë°ì´í„° ë¡œë“œ: ì¤‘ë³µ ìƒì‚° ë°©ì§€"""
    if not STATE_PATH.exists(): 
        return set()
    try: 
        return set(str(bid) for bid in json.loads(STATE_PATH.read_text(encoding="utf-8")).get("processed_ids", []))
    except Exception as e:
        logger.error(f"State load failed: {e}")
        return set()

def fetch_work_queue():
    """7ë§Œ ê¶Œ ëª©ë¡ ì¤‘ ê³ ê°€ì¹˜ ìì‚° ì¶”ì¶œ (ì œëª©+ì €ì ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    processed = load_processed_ids()
    
    try:
        resp = requests.get(INDEX_URL, timeout=30)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.critical(f"[FATALITY] Index fetch failed: {e}")
        return []
    
    resp.encoding = 'utf-8'
    reader = csv.DictReader(resp.text.splitlines())
    
    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    fieldnames = {k.strip(): k for k in (reader.fieldnames or [])}
    text_key = fieldnames.get('Text#')
    title_key = fieldnames.get('Title')
    author_key = fieldnames.get('Authors')
    subjects_key = fieldnames.get('Subjects')
    
    # Downloads ì»¬ëŸ¼ íƒìƒ‰
    possible_keys = ['Downloads', 'Download Count', 'downloads']
    actual_key = next((fieldnames.get(k) for k in possible_keys if fieldnames.get(k)), None)
    
    all_books = list(reader)
    if actual_key:
        all_books.sort(key=lambda x: int(x.get(actual_key, 0) or 0), reverse=True)
    
    queue = []
    for row in all_books:
        book_id = row.get(text_key, '').strip() if text_key else ''
        if book_id and book_id not in processed:
            queue.append({
                "id": book_id, 
                "title": row.get(title_key, 'Unknown Title').strip(),
                "author": row.get(author_key, 'Unknown Author').strip() if author_key else 'Unknown Author',
                "subjects": row.get(subjects_key, '').strip() if subjects_key else ''
            })
        if len(queue) >= MAX_BOOKS: 
            break
    return queue

def get_ai_insight(title: str, author: str, subjects: str) -> Optional[str]:
    """
    [Antifragile AI í˜¸ì¶œ] ì¬ì‹œë„ + ì§€ìˆ˜ ë°±ì˜¤í”„ + ì—ëŸ¬ íƒ€ì…ë³„ ê²©ë¦¬
    """
    if not GEMINI_API_KEY:
        logger.warning(f"API Key missing for '{title}'")
        return None
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
    headers = {'Content-Type': 'application/json'}
    
    context = f"Author: {author}" if author != 'Unknown Author' else ""
    if subjects:
        context += f" | Genre/Subjects: {subjects[:100]}"
    
    prompt = {
        "contents": [{
            "parts": [{
                "text": (
                    f"Book Title: '{title}'\n"
                    f"{context}\n\n"
                    f"Task: Extract ONE UNIQUE strategic business insight from THIS SPECIFIC BOOK "
                    f"for global financial architecture optimization. "
                    f"Do NOT use generic advice like 'optimize resources' or 'be strategic'. "
                    f"Reflect the book's SPECIFIC themes, plot, or philosophical arguments. "
                    f"Must be actionable and distinctive to THIS book. "
                    f"Keep it under 200 characters in English."
                )
            }]
        }]
    }
    
    # [í•µì‹¬] ì¬ì‹œë„ ë¡œì§ with ì§€ìˆ˜ ë°±ì˜¤í”„
    for attempt in range(MAX_RETRIES):
        try:
            # Rate Limit ë°©ì–´: ìš”ì²­ 'ì „' ëŒ€ê¸°
            if attempt > 0:
                backoff_delay = RATE_LIMIT_DELAY * (RETRY_BACKOFF_BASE ** (attempt - 1))
                logger.info(f"Retry {attempt}/{MAX_RETRIES} for '{title}' after {backoff_delay:.1f}s")
                time.sleep(backoff_delay)
            else:
                time.sleep(RATE_LIMIT_DELAY)
            
            response = requests.post(url, headers=headers, json=prompt, timeout=20)
            
            # [ì—ëŸ¬ íƒ€ì…ë³„ ë¶„ê¸°]
            if response.status_code == 200:
                insight = response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
                logger.info(f"âœ… AI success for '{title[:30]}'")
                return insight
            
            elif response.status_code == 429:
                # Rate Limit: ì¬ì‹œë„ ê°€ëŠ¥
                logger.warning(f"â³ Rate Limit hit for '{title}' (attempt {attempt+1})")
                if attempt < MAX_RETRIES - 1:
                    continue  # ì¬ì‹œë„
                else:
                    logger.error(f"âŒ Rate Limit exhausted for '{title}'")
                    return None
            
            elif response.status_code >= 500:
                # Server Error: ì¬ì‹œë„ ê°€ëŠ¥
                logger.warning(f"ğŸ”§ Server error {response.status_code} for '{title}' (attempt {attempt+1})")
                if attempt < MAX_RETRIES - 1:
                    continue
                else:
                    logger.error(f"âŒ Server errors exhausted for '{title}'")
                    return None
            
            elif response.status_code == 403:
                # Forbidden: API í‚¤ ë¬¸ì œ, ì¬ì‹œë„ ë¶ˆê°€
                logger.critical(f"ğŸ›‘ API Key invalid for '{title}': {response.text[:100]}")
                return None
            
            else:
                # ê¸°íƒ€ í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬: ì¬ì‹œë„ ë¶ˆê°€
                logger.error(f"âŒ HTTP {response.status_code} for '{title}': {response.text[:100]}")
                return None
                
        except requests.Timeout:
            logger.warning(f"â±ï¸ Timeout for '{title}' (attempt {attempt+1})")
            if attempt < MAX_RETRIES - 1:
                continue
            else:
                logger.error(f"âŒ Timeout exhausted for '{title}'")
                return None
        
        except requests.RequestException as e:
            logger.error(f"ğŸŒ Network error for '{title}': {e}")
            if attempt < MAX_RETRIES - 1:
                continue
            else:
                return None
        
        except (KeyError, IndexError, json.JSONDecodeError) as e:
            # ì‘ë‹µ íŒŒì‹± ì—ëŸ¬: ì¬ì‹œë„ ë¶ˆê°€
            logger.error(f"ğŸ” Response parse error for '{title}': {e}")
            return None
        
        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: ê²©ë¦¬
            logger.critical(f"ğŸ’¥ Unexpected error for '{title}': {type(e).__name__} - {e}")
            return None
    
    return None  # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨

def generate_asset(book_id: str, title: str, author: str, subjects: str) -> Optional[Dict[str, Any]]:
    """
    [Step 2] ë°ì´í„° êµ¬ì¡° ë‚´ ì¶œì²˜ ëª…ì‹œ ë° AI í†µì°° ì£¼ì…
    """
    # AI ì§€ëŠ¥ ì£¼ì… (Antifragile í˜¸ì¶œ)
    insight = get_ai_insight(title, author, subjects)
    
    # [í•µì‹¬] AI ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ (State ì˜¤ì—¼ ë°©ì§€)
    if insight is None:
        logger.warning(f"âš ï¸ Skipping asset for '{title}' due to AI failure")
        return None
    
    safe_title = str(title or "Unknown")[:80]
    safe_author = str(author or "Unknown")[:50]
    
    return {
        "book_id": str(book_id),
        "source_book": safe_title,
        "source_author": safe_author,
        "audience": "professional",
        "irreversible_insight": insight,
        "cards": [
            f"Structural Audit: Analyze '{safe_title[:30]}' patterns",
            f"Strategic Pivot: Apply {safe_author}'s framework", 
            "Scalable Growth: Standardize architecture"
        ],
        "quiz": [
            {"q": f"Core insight of '{safe_title[:30]}'?", "a": "Book-specific optimization"},
            {"q": f"Who wrote this?", "a": safe_author},
            {"q": "Application?", "a": "Financial architecture"}
        ],
        "script_60s": f"AI-powered insight from '{safe_title}' by {safe_author}.",
        "keywords": ["AI-Insight", safe_author.split()[0] if safe_author else "Strategy", "Book-Analysis"]
    }

def generate_sitemap(processed_ids):
    """ëª¨ë“  ìì‚°ì„ êµ¬ê¸€ì— ì‹ ê³ í•˜ê¸° ìœ„í•œ sitemap.xml ìƒì„±"""
    base_url = "https://pauljangtv-create.github.io/gutenberg-galaxy/"
    sitemap = ['<?xml version="1.0" encoding="UTF-8"?>', '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">']
    
    sitemap.append(f"<url><loc>{base_url}</loc><priority>1.0</priority></url>")
    
    for bid in list(processed_ids)[-5000:]:
        sitemap.append(f"<url><loc>{base_url}?id={bid}</loc><priority>0.8</priority></url>")
    
    sitemap.append('</urlset>')
    Path("sitemap.xml").write_text("\n".join(sitemap), encoding="utf-8")
    Path("robots.txt").write_text(f"User-agent: *\nAllow: /\nSitemap: {base_url}sitemap.xml", encoding="utf-8")
    logger.info("âœ… Sitemap generated for SEO")

def main():
    """
    [Antifragile Control System]
    HG3: Cost Guard with AI API validation
    """
    
    # --- [HG3] COST GUARD START (DO NOT REMOVE) ---
    PAID_LLM_ENABLED = bool(GEMINI_API_KEY)
    MAX_TOTAL_COST = 10.0
    current_estimated_cost = 0.0
    
    if not GEMINI_API_KEY:
        logger.critical("ğŸ›‘ [FATALITY] GEMINI_API_KEY missing. System freeze.")
        print("ğŸ’¡ Set GitHub Secret: GEMINI_API_KEY")
        sys.exit(1)
    
    if PAID_LLM_ENABLED and current_estimated_cost > MAX_TOTAL_COST:
        logger.critical("ğŸ›‘ [FATALITY] Cost threshold exceeded.")
        sys.exit(1)
    # --- [HG3] COST GUARD END ---

    logger.info(f"ğŸ›¡ï¸ [HG3 PASS] Risk/Cost safety verified: ${current_estimated_cost}")
    logger.info(f"ğŸ¤– AI Mode: {'Enabled (Antifragile)' if PAID_LLM_ENABLED else 'Disabled'}")
    logger.info(f"âš™ï¸ Rate Limit: {RATE_LIMIT_RPM} RPM (delay: {RATE_LIMIT_DELAY:.1f}s)")

    # 1. ìƒì‚° ì¤€ë¹„ ë° ìƒíƒœ ë¡œë“œ
    queue = fetch_work_queue()
    processed_ids = list(load_processed_ids())
    
    if not queue:
        logger.info("âš ï¸ No pending tasks. System idling.")
        return

    logger.info(f"ğŸ“‹ Queue size: {len(queue)} books")

    # 2. AI ê¸°ë°˜ ë§ì¶¤í˜• ìƒì‚° ë£¨í”„ (Isolating Architecture)
    success_count = 0
    failure_count = 0
    
    for item in queue:
        try:
            logger.info(f"ğŸ”„ Processing: {item['id']} - '{item['title'][:40]}' by {item['author'][:30]}")
            
            # AIë¡œ ê°œë³„ ìì‚° ìƒì„± (None ë°˜í™˜ ì‹œ ê±´ë„ˆëœ€)
            data = generate_asset(
                item['id'], 
                item['title'], 
                item['author'],
                item['subjects']
            )
            
            # [í•µì‹¬] AI ì‹¤íŒ¨ ì‹œ State ì˜¤ì—¼ ë°©ì§€
            if data is None:
                logger.warning(f"â­ï¸ Skipped: {item['id']} (AI failure)")
                failure_count += 1
                continue  # processed_idsì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ!
            
            validate(instance=data, schema=SCHEMA)
            
            # HG4: ì••ì¶• ì €ì¥ ë° ìì‚°í™”
            file_path = OUT_DIR / f"{item['id']}.json.gz"
            with gzip.open(file_path, "wb") as f:
                f.write(json.dumps(data, ensure_ascii=False).encode("utf-8"))
            
            # [í•µì‹¬] ì„±ê³µ ì‹œì—ë§Œ State ì—…ë°ì´íŠ¸
            processed_ids.append(item['id'])
            success_count += 1
            logger.info(f"âœ… Produced: {item['id']} | Insight: {data['irreversible_insight'][:60]}...")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error for {item['id']}: {type(e).__name__} - {e}")
            failure_count += 1
            continue  # ë¦¬ìŠ¤í¬ ì „ì´ ë°©ì§€

    # 3. ìƒíƒœ ê¸°ë¡ ë° ë™ê¸°í™” (ì„±ê³µí•œ ê²ƒë§Œ)
    final_state = {"processed_ids": sorted(list(set(processed_ids)))}
    STATE_PATH.write_text(json.dumps(final_state, indent=2), encoding="utf-8")
    
    # 4. SEO: Sitemap ìƒì„±
    generate_sitemap(processed_ids)
    
    # 5. ìµœì¢… ë¦¬í¬íŠ¸
    logger.info("=" * 60)
    logger.info(f"ğŸ‰ Production complete")
    logger.info(f"âœ… Success: {success_count} assets")
    logger.info(f"âŒ Failures: {failure_count} assets")
    logger.info(f"ğŸ“Š Success Rate: {success_count/(success_count+failure_count)*100:.1f}%")
    logger.info("=" * 60)

if __name__ == "__main__":
    main()
