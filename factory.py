import gzip
import json
import os
import requests
import pandas as pd
from pathlib import Path
from jsonschema import validate

# [ì„¤ì •] ê¸€ë¡œë²Œ ì¸ë±ìŠ¤ ë° ìì‚° ê²½ë¡œ
INDEX_URL = "https://www.gutenberg.org/cache/epub/feeds/pg_catalog.csv"
STATE_PATH = Path("state.json")
OUT_DIR = Path("products")
OUT_DIR.mkdir(exist_ok=True)
SCHEMA = json.loads(Path("schema.json").read_text(encoding="utf-8"))

# HG1: í•˜ë£¨ ìƒì‚°ëŸ‰ ì œí•œ (Actions ë¬´ë£Œ ì¿¼í„° ìµœì í™”)
MAX_BOOKS = 200 

def load_processed_ids():
    """ìƒíƒœ ë°ì´í„° ë¡œë“œ: ì¤‘ë³µ ìƒì‚° ë°©ì§€"""
    if not STATE_PATH.exists(): return set()
    try:
        data = json.loads(STATE_PATH.read_text(encoding="utf-8"))
        return set(str(bid) for bid in data.get("processed_ids", []))
    except: return set()

def fetch_work_queue():
    """7ë§Œ ê¶Œ ì¤‘ ë¯¸ì²˜ë¦¬ëœ ì¸ê¸° ë„ì„œ 200ê¶Œ ì¶”ì¶œ"""
    processed = load_processed_ids()
    print(f"ğŸ” Accessing Global Index: {INDEX_URL}")
    df = pd.read_csv(INDEX_URL)
    
    # ë‹¤ìš´ë¡œë“œ ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ê°€ì¥ ì‹œì¥ì„± ë†’ì€ ê³ ì „ ìš°ì„ ìˆœìœ„)
    df = df.sort_values(by='Downloads', ascending=False)
    
    queue = []
    for _, row in df.iterrows():
        book_id = str(row['Text#'])
        if book_id not in processed:
            queue.append({
                "id": book_id, 
                "title": row['Title'],
                "authors": row['Authors']
            })
        if len(queue) >= MAX_BOOKS: break
    return queue

def get_remote_text(book_id):
    """êµ¬í…ë² ë¥´í¬ ë¯¸ëŸ¬ ì„œë²„ì—ì„œ ì›ì¬ë£Œ ì§ì ‘ ìˆ˜ê¸‰"""
    url = f"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt"
    try:
        resp = requests.get(url, timeout=10)
        return resp.text if resp.status_code == 200 else None
    except: return None

def generate_asset(book_id, title):
    """[ìƒì‚° ë¡œì§] í…ìŠ¤íŠ¸ ë¶„ì„ ë° ê·œê²©í™”ëœ ìƒí’ˆ ìƒì„±"""
    # í˜„ì¬ëŠ” ì¸í”„ë¼ ê²€ì¦ì„ ìœ„í•´ ê·œê²©ì— ë§ì¶˜ í´ë°± ë°ì´í„° ìƒì„±
    # ì¶”í›„ PAID_LLM_ENABLED ì„¤ì •ì„ í†µí•´ ì‹¤ì œ AI í†µì°°ë¡œ êµì²´ ê°€ëŠ¥
    return {
        "book_id": book_id,
        "audience": "professional",
        "irreversible_insight": f"Strategic analysis of '{title}' for global optimization.",
        "cards": ["Assess Core Strategy", "Execute Micro-experiment", "Validate Results"],
        "quiz": [
            {"q": "What is the primary goal?", "a": "Strategic Optimization"},
            {"q": "How to manage risk?", "a": "Identify Fatalities"},
            {"q": "Current Phase?", "a": "Automated Production"}
        ],
        "script_60s": f"Discover the hidden patterns in {title}.",
        "keywords": ["strategy", "classics", "optimization"]
    }

def main():
    queue = fetch_work_queue()
    processed_ids = list(load_processed_ids())
    
    print(f"ğŸš€ Starting Production Line: {len(queue)} items in queue.")
    
    for item in queue:
        try:
            data = generate_asset(item['id'], item['title'])
            validate(instance=data, schema=SCHEMA) # HG2: í’ˆì§ˆ ê²€ìˆ˜
            
            # HG4: ì••ì¶• ì €ì¥
            with gzip.open(OUT_DIR / f"{item['id']}.json.gz", "wb") as f:
                f.write(json.dumps(data, ensure_ascii=False).encode("utf-8"))
            
            processed_ids.append(item['id'])
            print(f"âœ… Produced: {item['id']} - {item['title'][:30]}")
        except Exception as e:
            print(f"âŒ Skip {item['id']}: {e}")
            
    # ìµœì¢… ìƒíƒœ ê¸°ë¡
    STATE_PATH.write_text(
        json.dumps({"processed_ids": sorted(list(set(processed_ids)))}, indent=2),
        encoding="utf-8"
    )

if __name__ == "__main__":
    main()
